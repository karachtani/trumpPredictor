
===================================================
LAG 0
('best parameters found: ', {'activation': 'logistic', 'alpha': 0.001, 'batch_size': 'auto', 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'max_iter': 5000, 'momentum': 0.9, 'solver': 'lbfgs'})
MLPClassifier(activation='logistic', alpha=0.001, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100,), learning_rate='constant',
              learning_rate_init=0.001, max_iter=5000, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,
              validation_fraction=0, verbose=False, warm_start=False)
Classification Report
              precision    recall  f1-score   support

          -1       0.45      0.20      0.28      2121
           1       0.54      0.79      0.64      2533

    accuracy                           0.52      4654
   macro avg       0.50      0.50      0.46      4654
weighted avg       0.50      0.52      0.48      4654

Confusion Matrix
[[ 429 1692]
 [ 527 2006]]
Accuracy:
0.5232058444348947
===================================================
LAG 1
('best parameters found: ', {'activation': 'logistic', 'alpha': 0.0001, 'batch_size': 'auto', 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'max_iter': 5000, 'momentum': 0.9, 'solver': 'lbfgs'})
MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',
              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100,), learning_rate='constant',
              learning_rate_init=0.001, max_iter=5000, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,
              validation_fraction=0, verbose=False, warm_start=False)
Classification Report
              precision    recall  f1-score   support

          -1       0.46      0.59      0.52      2031
           1       0.60      0.47      0.53      2623

    accuracy                           0.52      4654
   macro avg       0.53      0.53      0.52      4654
weighted avg       0.54      0.52      0.52      4654

Confusion Matrix
[[1202  829]
 [1386 1237]]
Accuracy:
0.5240653201547056
===================================================
LAG 2
('best parameters found: ', {'activation': 'logistic', 'alpha': 0.0001, 'batch_size': 'auto', 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'max_iter': 5000, 'momentum': 0.9, 'solver': 'lbfgs'})
MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',
              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100,), learning_rate='constant',
              learning_rate_init=0.001, max_iter=5000, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,
              validation_fraction=0, verbose=False, warm_start=False)
Classification Report
              precision    recall  f1-score   support

          -1       0.41      0.41      0.41      2013
           1       0.55      0.55      0.55      2641

    accuracy                           0.49      4654
   macro avg       0.48      0.48      0.48      4654
weighted avg       0.49      0.49      0.49      4654

Confusion Matrix
[[ 828 1185]
 [1179 1462]]
Accuracy:
0.492049849591749
===================================================
LAG 3
('best parameters found: ', {'activation': 'logistic', 'alpha': 0.01, 'batch_size': 'auto', 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'max_iter': 5000, 'momentum': 0.9, 'solver': 'lbfgs'})
MLPClassifier(activation='logistic', alpha=0.01, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100,), learning_rate='constant',
              learning_rate_init=0.001, max_iter=5000, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,
              validation_fraction=0, verbose=False, warm_start=False)
Classification Report
              precision    recall  f1-score   support

          -1       0.49      0.65      0.56      1986
           1       0.66      0.50      0.57      2668

    accuracy                           0.56      4654
   macro avg       0.57      0.57      0.56      4654
weighted avg       0.59      0.56      0.56      4654

Confusion Matrix
[[1292  694]
 [1336 1332]]
Accuracy:
0.5638160721959604
===================================================
LAG 4
('best parameters found: ', {'activation': 'logistic', 'alpha': 0.1, 'batch_size': 'auto', 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'max_iter': 5000, 'momentum': 0.9, 'solver': 'lbfgs'})
MLPClassifier(activation='logistic', alpha=0.1, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100,), learning_rate='constant',
              learning_rate_init=0.001, max_iter=5000, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,
              validation_fraction=0, verbose=False, warm_start=False)
Classification Report
              precision    recall  f1-score   support

          -1       0.42      0.10      0.17      1815
           1       0.61      0.91      0.73      2839

    accuracy                           0.60      4654
   macro avg       0.52      0.51      0.45      4654
weighted avg       0.54      0.60      0.51      4654

Confusion Matrix
[[ 190 1625]
 [ 258 2581]]
Accuracy:
0.5954018048990116
===================================================
LAG 5
('best parameters found: ', {'activation': 'logistic', 'alpha': 0.1, 'batch_size': 'auto', 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'max_iter': 5000, 'momentum': 0.9, 'solver': 'lbfgs'})
MLPClassifier(activation='logistic', alpha=0.1, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100,), learning_rate='constant',
              learning_rate_init=0.001, max_iter=5000, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,
              validation_fraction=0, verbose=False, warm_start=False)
Classification Report
              precision    recall  f1-score   support

          -1       0.38      0.41      0.39      1995
           1       0.53      0.50      0.51      2659

    accuracy                           0.46      4654
   macro avg       0.45      0.45      0.45      4654
weighted avg       0.46      0.46      0.46      4654

Confusion Matrix
[[ 809 1186]
 [1338 1321]]
Accuracy:
0.45767082079931243
===================================================
LAG 6
('best parameters found: ', {'activation': 'logistic', 'alpha': 0.0001, 'batch_size': 'auto', 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'max_iter': 5000, 'momentum': 0.9, 'solver': 'lbfgs'})
MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',
              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100,), learning_rate='constant',
              learning_rate_init=0.001, max_iter=5000, momentum=0.9,
              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,
              validation_fraction=0, verbose=False, warm_start=False)
Classification Report
              precision    recall  f1-score   support

          -1       0.39      0.12      0.18      2071
           1       0.55      0.85      0.67      2583

    accuracy                           0.53      4654
   macro avg       0.47      0.49      0.43      4654
weighted avg       0.48      0.53      0.45      4654

Confusion Matrix
[[ 250 1821]
 [ 385 2198]]
Accuracy:
0.5259991405242802